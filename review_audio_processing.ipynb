{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import numpy as np\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "from functools import partial\n",
    "import librosa \n",
    "from tqdm import tqdm\n",
    "import multiprocessing\n",
    "from data_util import build_speaker_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>client_id</th>\n",
       "      <th>path</th>\n",
       "      <th>sentence</th>\n",
       "      <th>up_votes</th>\n",
       "      <th>down_votes</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>accent</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00c3f0e7c691ef30257d1bfa9adc410535b7ba3f48e344...</td>\n",
       "      <td>common_voice_en_18295850.mp3</td>\n",
       "      <td>The long-lived bridge still stands today.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>05ba9bb1a4ac391849fa4461547967768f4d7df8ee52d7...</td>\n",
       "      <td>common_voice_en_19967535.mp3</td>\n",
       "      <td>The cemetery is now managed by three trusts.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>fifties</td>\n",
       "      <td>male</td>\n",
       "      <td>african</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>06546553aed17027b4e638d4afb56f39b216026088cf40...</td>\n",
       "      <td>common_voice_en_17147389.mp3</td>\n",
       "      <td>Women form less than half of the group.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>0838a82655be5a61349c2d2d86b60c22b5b84fea9826cb...</td>\n",
       "      <td>common_voice_en_18127728.mp3</td>\n",
       "      <td>Sunburn can be avoided by applying sunscreen o...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>0899979e8d43a9faf448ddb5f4fc9a38a0fb4c120eaf34...</td>\n",
       "      <td>common_voice_en_17850951.mp3</td>\n",
       "      <td>Still waters run deep.</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>twenties</td>\n",
       "      <td>male</td>\n",
       "      <td>other</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             client_id  \\\n",
       "9    00c3f0e7c691ef30257d1bfa9adc410535b7ba3f48e344...   \n",
       "70   05ba9bb1a4ac391849fa4461547967768f4d7df8ee52d7...   \n",
       "81   06546553aed17027b4e638d4afb56f39b216026088cf40...   \n",
       "100  0838a82655be5a61349c2d2d86b60c22b5b84fea9826cb...   \n",
       "104  0899979e8d43a9faf448ddb5f4fc9a38a0fb4c120eaf34...   \n",
       "\n",
       "                             path  \\\n",
       "9    common_voice_en_18295850.mp3   \n",
       "70   common_voice_en_19967535.mp3   \n",
       "81   common_voice_en_17147389.mp3   \n",
       "100  common_voice_en_18127728.mp3   \n",
       "104  common_voice_en_17850951.mp3   \n",
       "\n",
       "                                              sentence  up_votes  down_votes  \\\n",
       "9            The long-lived bridge still stands today.         2           0   \n",
       "70        The cemetery is now managed by three trusts.         2           0   \n",
       "81             Women form less than half of the group.         2           0   \n",
       "100  Sunburn can be avoided by applying sunscreen o...         2           0   \n",
       "104                             Still waters run deep.         2           0   \n",
       "\n",
       "          age gender   accent  \n",
       "9    twenties   male      NaN  \n",
       "70    fifties   male  african  \n",
       "81   twenties   male       us  \n",
       "100  twenties   male      NaN  \n",
       "104  twenties   male    other  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "temp_dir = tempfile.gettempdir()\n",
    "# common_voice_path=os.path.abspath(\"../transaccent/data/en\")\n",
    "common_voice_path=os.path.abspath(\"/data\")\n",
    "audio_files_path = os.path.join(common_voice_path, \"clips\")\n",
    "data_file_filename = \"validated.tsv\"\n",
    "data_file_path = os.path.join(common_voice_path, data_file_filename)\n",
    "\n",
    "# Retreive records\n",
    "df = pd.read_csv(data_file_path, sep='\\t', low_memory=False)\n",
    "df = df[(df['down_votes'] < 1) & (df['gender'] == 'male') & (df['up_votes'] > 1)]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 576 records with indian accents and 7827 records with us accents\n"
     ]
    }
   ],
   "source": [
    "indian_df = build_speaker_dataset(df[(df['accent'] == 'indian') ], 1)\n",
    "us_df = build_speaker_dataset(df[df['accent'] == 'us'], 1)\n",
    "print(f\"Retrieved {len(indian_df)} records with indian accents and {len(us_df)} records with us accents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# hop=192               #hop size (window size = 6*hop)\n",
    "# sr=16000              #sampling rate\n",
    "# min_level_db=-100     #reference values to normalize data\n",
    "# ref_level_db=20\n",
    "\n",
    "# shape=24              #length of time axis of split specrograms to feed to generator            \n",
    "# vec_len=128           #length of vector generated by siamese vector\n",
    "# # bs = 16               #batch size\n",
    "# bs = 32               #batch size\n",
    "# delta = 2.            #constant for siamese loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from addict import Dict\n",
    "\n",
    "def intialise_hparams(values):\n",
    "    hparams = Dict(values)\n",
    "    \n",
    "    hparams.n_fft = 6 * hparams.hop # length of the windowed signal after padding with zeros\n",
    "    hparams.win_length = 6 * hparams.hop\n",
    "    hparams.max_spec_length = 10 * hparams.shape \n",
    "\n",
    "    return hparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152\n",
      "40\n",
      "1152\n",
      "240\n",
      "24064\n"
     ]
    }
   ],
   "source": [
    "from yaml import load, dump\n",
    "try:\n",
    "    from yaml import CLoader as Loader, CDumper as Dumper\n",
    "except ImportError:\n",
    "    from yaml import Loader, Dumper\n",
    "\n",
    "with open(\"hparams.yaml\", \"r\") as fp:\n",
    "    hparams_yaml = load(fp, Loader=Loader)\n",
    "\n",
    "hparams = intialise_hparams(hparams_yaml)\n",
    "\n",
    "print(hparams.n_fft)\n",
    "print(hparams.trim_top_db)\n",
    "print(hparams.win_length)\n",
    "print(hparams.max_spec_length)\n",
    "print(hparams.max_audio_samples)\n",
    "print(hparams.generator.downscale.filter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def pad_audio_with_silence(wav, pad_value, hparams):\n",
    "    \"\"\" We want to ensure that the size of all the audio samples are the same.\n",
    "    If the audio file is smaller than the desired size then we will pad the ending with 0s\n",
    "    If the audio file is larger than the desired size then we disregard the excess content\n",
    "    \"\"\"\n",
    "    if len(wav) < hparams.max_audio_samples:\n",
    "        pad_samples = hparams.max_audio_samples - len(wav)\n",
    "        wav = np.pad(wav, (0, pad_samples), mode='constant', constant_values=pad_value)\n",
    "    else: \n",
    "        wav = wav[:hparams.max_audio_samples]\n",
    "    return wav\n",
    "\n",
    "def trim_silence(wav, hparams):\n",
    "    \"\"\" We want to ensure the beginning of the audio file is not silent\"\"\"\n",
    "    return librosa.effects.trim(wav, top_db=hparams.trim_top_db, frame_length=hparams.trim_fft_size, hop_length=hparams.trim_hop_size)[0]\n",
    "\n",
    "def load_audio(audio_file_name, hparams):\n",
    "    \"\"\"We use the librosa library to retrieve generate the representation of the audio file\n",
    "    The sample rate used is specified as part of the hyperparameters.\n",
    "    The function works for both mp3 and wav files\n",
    "    \"\"\"\n",
    "    # The loading of mp3 files throw a warning, due to the volume of files we ignore this warning\n",
    "    import warnings\n",
    "    warnings.filterwarnings('ignore')\n",
    "    y, _= librosa.load(audio_file_name, sr=hparams.sample_rate)\n",
    "    return y\n",
    "\n",
    "def convert_mp3_to_signal(audio_file_name, hparams):\n",
    "    \"\"\"Generate the signal representation of a single audio file\"\"\"\n",
    "    y = load_audio(audio_file_name, hparams)\n",
    "    if hparams.trim_silence:\n",
    "        y = trim_silence(y, hparams)\n",
    "    # pad to try to make all the audio files the same length\n",
    "    y = pad_audio_with_silence(y, pad_value=0., hparams=hparams)\n",
    "    return y\n",
    "\n",
    "def process_audio(audio_path, hparams):\n",
    "    \"\"\"Helper function to generate the numpy array representation of the audio files\"\"\"\n",
    "    return np.array(convert_mp3_to_signal(os.path.join(audio_files_path, audio_path), hparams))\n",
    "\n",
    "def audio_array_for_accent(df, accent, hparams, limit_size=-1, tqdm=lambda x: x):\n",
    "\n",
    "    import multiprocessing\n",
    "    import numpy as np\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    \n",
    "    input_mp3_files = df[df['accent'] == accent]['path'].to_numpy() \n",
    "    \n",
    "    if limit_size > 0:\n",
    "        print(f\"Retrieved {len(input_mp3_files)} will only be using {limit_size}\")\n",
    "        input_mp3_files = input_mp3_files[:limit_size]\n",
    "    \n",
    "    executor = ProcessPoolExecutor(max_workers=cpu_count)\n",
    "    futures = []\n",
    "    index = 1\n",
    "    \n",
    "    for filepath in input_mp3_files:\n",
    "        futures.append(executor.submit(partial(process_audio, filepath, hparams)))\n",
    "        index += 1\n",
    "    \n",
    "    return np.array([future.result() for future in tqdm(futures) if future.result() is not None])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def melspecfunc(waveform, hparams):\n",
    "    return librosa.feature.melspectrogram(y=waveform, \n",
    "                                          sr=hparams.sample_rate, \n",
    "                                          n_fft=hparams.n_fft, \n",
    "                                          win_length=hparams.win_length,\n",
    "                                          hop_length=hparams.hop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def normalize(S, hparams):\n",
    "    return np.clip((((S - hparams.min_level_db) / -hparams.min_level_db)*2.)-1., -1, 1)\n",
    "\n",
    "def prep(wv, hparams):\n",
    "    S = np.array(melspecfunc(wv, hparams))\n",
    "    S = librosa.power_to_db(S)-hparams.ref_level_db\n",
    "    return normalize(S, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def process_wav(wav, hparams):\n",
    "    S = np.array(prep(wav, hparams), dtype=np.float32)\n",
    "    return np.expand_dims(S, -1)\n",
    "\n",
    "def tospec(wvs, hparams):\n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(cpu_count)\n",
    "    specs = pool.map(partial(process_wav, hparams=hparams), wvs)\n",
    "    \n",
    "    return specs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# split into equal chunk size\n",
    "def split_spec(spec, chunk_size):\n",
    "    return [spec[i * chunk_size: (i+1) * chunk_size] for i in range(int(np.ceil(len(spec) / chunk_size)))]\n",
    "\n",
    "def splitcut(data, hparams):\n",
    "    chunk_size = hparams.max_spec_length                                                             #max spectrogram length\n",
    "    \n",
    "    cpu_count = multiprocessing.cpu_count()\n",
    "    pool = multiprocessing.Pool(cpu_count)\n",
    "    splits = pool.map(partial(split_spec,chunk_size=chunk_size), data)\n",
    "    \n",
    "    return np.array(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def save_generated_representation(dir_name, spec, data):\n",
    "    try:\n",
    "        filename = f'{dir_name}/data.npz'\n",
    "        if os.path.exists(filename):\n",
    "            os.remove(filename)\n",
    "        np.savez(filename, spec=spec, data=data)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def generate_representations(df, accent, output_dir, hparams):\n",
    "    \"\"\"Generate the representation of the audio files used by the DNNs\"\"\"\n",
    "    # generate the signal representation for all the audio files restricted by the accent specified\n",
    "    wv_all = audio_array_for_accent(df, accent, hparams, limit_size=len(indian_df))\n",
    "    # Generate the the spectrogram representation\n",
    "    spec = tospec(wv_all, hparams)\n",
    "    # Split the spetrograms into equal size chucks\n",
    "    data = splitcut(spec, hparams)\n",
    "    # save the generated representations\n",
    "    os.makedirs(indian_dir, exist_ok = True)\n",
    "    save_generated_representation(output_dir, spec, data)\n",
    "    return spec, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "indian_dir = '/data/wav/v3/indian'\n",
    "os.makedirs(indian_dir, exist_ok = True)\n",
    "us_dir = '/data/wav/v3/us'\n",
    "os.makedirs(us_dir, exist_ok = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "input_accent = 'indian'\n",
    "target_accent = 'us'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieved 576 will only be using 576\n"
     ]
    }
   ],
   "source": [
    "aspec, adata = generate_representations(indian_df, input_accent, indian_dir, hparams)\n",
    "bspec, adabdatata = generate_representations(us_df, target_accent, us_dir, hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Creating Tensorflow Datasets\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def proc(x):\n",
    "    return tf.image.random_crop(x, size=[hop, 3*shape, 1])\n",
    "\n",
    "dsa = tf.data.Dataset.from_tensor_slices(adata)\n",
    "    .repeat(hparams.num_dataset_repetitions)\n",
    "    .map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .shuffle(hparams.shuffle_buffer_size)\n",
    "    .batch(hparams.batch_size, drop_remainder=hparams.batch_drop_remainder)\n",
    "\n",
    "dsb = tf.data.Dataset.from_tensor_slices(bdata)\n",
    "    .repeat(hparams.num_dataset_repetitions)\n",
    "    .map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "    .shuffle(hparams.shuffle_buffer_size)\n",
    "    .batch(hparams.batch_size, drop_remainder=hparams.batch_drop_remainder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.utils import conv_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import sparse_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import standard_ops\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import tensor_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (tf.norm(v) + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Adding Spectral Normalization to convolutional layers\n",
    "# each convolutional filter of both G and D is spectrally normalized as this greatly improves training stability - https://arxiv.org/abs/1802.05957\n",
    "\n",
    "class ConvSN2D(tf.keras.layers.Conv2D):\n",
    "\n",
    "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
    "        super(ConvSN2D, self).__init__(filters, kernel_size, **kwargs)\n",
    "        self.power_iterations = power_iterations\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ConvSN2D, self).build(input_shape)\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        self.u = self.add_weight(self.name + '_u',\n",
    "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
    "            initializer=tf.initializers.RandomNormal(0, 1),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
    "        for _ in range(self.power_iterations):\n",
    "\n",
    "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
    "            new_u = l2normalize(tf.matmul(new_v, W))\n",
    "            \n",
    "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
    "        W_bar = W/sigma\n",
    "\n",
    "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
    "            W_bar = tf.reshape(W_bar, W_shape)\n",
    "\n",
    "        return W_bar\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
    "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
    "        outputs = self._convolution_op(inputs, new_kernel)\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.data_format == 'channels_first':\n",
    "                    outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
    "            else:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class ConvSN2DTranspose(tf.keras.layers.Conv2DTranspose):\n",
    "\n",
    "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
    "        super(ConvSN2DTranspose, self).__init__(filters, kernel_size, **kwargs)\n",
    "        self.power_iterations = power_iterations\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ConvSN2DTranspose, self).build(input_shape)\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        self.u = self.add_weight(self.name + '_u',\n",
    "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
    "            initializer=tf.initializers.RandomNormal(0, 1),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
    "        for _ in range(self.power_iterations):\n",
    "\n",
    "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
    "            new_u = l2normalize(tf.matmul(new_v, W))\n",
    "            \n",
    "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
    "        W_bar = W/sigma\n",
    "\n",
    "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
    "            W_bar = tf.reshape(W_bar, W_shape)\n",
    "\n",
    "        return W_bar\n",
    "\n",
    "    def call(self, inputs):\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
    "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
    "\n",
    "        inputs_shape = array_ops.shape(inputs)\n",
    "        batch_size = inputs_shape[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            h_axis, w_axis = 2, 3\n",
    "        else:\n",
    "            h_axis, w_axis = 1, 2\n",
    "\n",
    "        height, width = inputs_shape[h_axis], inputs_shape[w_axis]\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.strides\n",
    "\n",
    "        if self.output_padding is None:\n",
    "            out_pad_h = out_pad_w = None\n",
    "        else:\n",
    "            out_pad_h, out_pad_w = self.output_padding\n",
    "\n",
    "        out_height = conv_utils.deconv_output_length(height,\n",
    "                                                    kernel_h,\n",
    "                                                    padding=self.padding,\n",
    "                                                    output_padding=out_pad_h,\n",
    "                                                    stride=stride_h,\n",
    "                                                    dilation=self.dilation_rate[0])\n",
    "        out_width = conv_utils.deconv_output_length(width,\n",
    "                                                    kernel_w,\n",
    "                                                    padding=self.padding,\n",
    "                                                    output_padding=out_pad_w,\n",
    "                                                    stride=stride_w,\n",
    "                                                    dilation=self.dilation_rate[1])\n",
    "        if self.data_format == 'channels_first':\n",
    "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
    "        else:\n",
    "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
    "\n",
    "        output_shape_tensor = array_ops.stack(output_shape)\n",
    "        outputs = K.conv2d_transpose(\n",
    "            inputs,\n",
    "            new_kernel,\n",
    "            output_shape_tensor,\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format,\n",
    "            dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if not context.executing_eagerly():\n",
    "            out_shape = self.compute_output_shape(inputs.shape)\n",
    "            outputs.set_shape(out_shape)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(\n",
    "              outputs,\n",
    "              self.bias,\n",
    "              data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class DenseSN(tf.keras.layers.Dense):\n",
    "    def build(self, input_shape):\n",
    "        super(DenseSN, self).build(input_shape)\n",
    "\n",
    "        self.u = self.add_weight(self.name + '_u',\n",
    "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
    "            initializer=tf.initializers.RandomNormal(0, 1),\n",
    "            trainable=False)\n",
    "        \n",
    "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
    "        new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
    "        new_u = l2normalize(tf.matmul(new_v, W))\n",
    "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
    "        W_bar = W/sigma\n",
    "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
    "            W_bar = tf.reshape(W_bar, W_shape)\n",
    "        return W_bar\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
    "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
    "        rank = len(inputs.shape)\n",
    "        if rank > 2:\n",
    "            outputs = standard_ops.tensordot(inputs, new_kernel, [[rank - 1], [0]])\n",
    "            if not context.executing_eagerly():\n",
    "                shape = inputs.shape.as_list()\n",
    "                output_shape = shape[:-1] + [self.units]\n",
    "                outputs.set_shape(output_shape)\n",
    "        else:\n",
    "            inputs = math_ops.cast(inputs, self._compute_dtype)\n",
    "            if K.is_sparse(inputs):\n",
    "                outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, new_kernel)\n",
    "            else:\n",
    "                outputs = gen_math_ops.mat_mul(inputs, new_kernel)\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Networks Architecture\n",
    "\n",
    "\n",
    "def conv2d(layer_input, \n",
    "           filters, \n",
    "           kernel_size=4, \n",
    "           strides=2, \n",
    "           padding='same', \n",
    "           leaky=True, \n",
    "           bnorm=True, \n",
    "           sn=True, \n",
    "           init_fun=tf.keras.initializers.he_uniform()):\n",
    "    \n",
    "    if leaky:\n",
    "        Activ = LeakyReLU(alpha=0.2)\n",
    "    else:\n",
    "        Activ = ReLU()\n",
    "    if sn:\n",
    "        d = ConvSN2D(\n",
    "            filters, \n",
    "            kernel_size=kernel_size, \n",
    "            strides=strides, \n",
    "            padding=padding, \n",
    "            kernel_initializer=init_fun, \n",
    "            use_bias=False)(layer_input)\n",
    "    else:\n",
    "        d = Conv2D(\n",
    "            filters, \n",
    "            kernel_size=kernel_size, \n",
    "            strides=strides, \n",
    "            padding=padding, \n",
    "            kernel_initializer=init_fun, \n",
    "            use_bias=False)(layer_input)\n",
    "    \n",
    "    if bnorm:\n",
    "        d = BatchNormalization()(d)\n",
    "\n",
    "    d = Activ(d)\n",
    "    return d\n",
    "\n",
    "def deconv2d(layer_input, \n",
    "             layer_res, \n",
    "             filters, \n",
    "             kernel_size=4, \n",
    "             conc=True, \n",
    "             scalev=False, \n",
    "             bnorm=True, \n",
    "             up=True, \n",
    "             padding='same', \n",
    "             strides=2, \n",
    "             init_fun=tf.keras.initializers.he_uniform()):\n",
    "    if up:\n",
    "        u = UpSampling2D((1,2))(layer_input)\n",
    "        u = ConvSN2D(\n",
    "            filters, \n",
    "            kernel_size, \n",
    "            strides=(1,1), \n",
    "            kernel_initializer=init_fun, \n",
    "            use_bias=False, \n",
    "            padding=padding)(u)\n",
    "    else:\n",
    "        u = ConvSN2DTranspose(\n",
    "            filters, \n",
    "            kernel_size, \n",
    "            strides=strides, \n",
    "            kernel_initializer=init_fun, \n",
    "            use_bias=False, \n",
    "            padding=padding)(layer_input)\n",
    "    \n",
    "    if bnorm:\n",
    "        u = BatchNormalization()(u)\n",
    "    u = LeakyReLU(alpha=0.2)(u)\n",
    "    if conc:\n",
    "        u = Concatenate()([u,layer_res])\n",
    "    return u\n",
    "\n",
    "#Extract function: splitting spectrograms\n",
    "def extract_image(im, hparams):\n",
    "    im1 = Cropping2D(((0,0), (0, 2*(im.shape[2]//3))))(im)\n",
    "    im2 = Cropping2D(((0,0), (im.shape[2]//3,im.shape[2]//3)))(im)\n",
    "    im3 = Cropping2D(((0,0), (2*(im.shape[2]//3), 0)))(im)\n",
    "    return im1,im2,im3\n",
    "\n",
    "#Assemble function: concatenating spectrograms\n",
    "def assemble_image(lsim, hparams):\n",
    "    im1,im2,im3 = lsim\n",
    "    imh = Concatenate(2)([im1,im2,im3])\n",
    "    return imh\n",
    "\n",
    "#U-NET style architecture\n",
    "def build_generator(input_shape, hparams):\n",
    "    h,w,c = input_shape\n",
    "    inp = Input(shape=input_shape)\n",
    "    \n",
    "    #downscaling\n",
    "    g0 = tf.keras.layers.ZeroPadding2D((0,1))(inp)\n",
    "    g1 = conv2d(g0, filters=hparams.generator.downscale.filter, kernel_size=(h,3), strides=1, padding='valid')\n",
    "    g2 = conv2d(g1, filters=hparams.generator.downscale.filter, kernel_size=(1,9), strides=(1,2))\n",
    "    g3 = conv2d(g2, filters=hparams.generator.downscale.filter, kernel_size=(1,7), strides=(1,2))\n",
    "    \n",
    "    #upscaling\n",
    "    g4 = deconv2d(g3, layer_res=g2, filters=hparams.generator.upscale.filter, kernel_size=(1,7), strides=(1,2))\n",
    "    g5 = deconv2d(g4, layer_res=g1, filters=hparams.generator.upscale.filter, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
    "    g6 = ConvSN2DTranspose(1, \n",
    "                           kernel_size=(h,1), \n",
    "                           strides=(1,1), \n",
    "                           kernel_initializer=tf.keras.initializers.he_uniform(), \n",
    "                           padding='valid', \n",
    "                           activation='tanh')(g5)\n",
    "    \n",
    "    return Model(inp,g6, name='G')\n",
    "\n",
    "#Siamese Network\n",
    "def build_siamese(input_shape, hparams):\n",
    "    h,w,c = input_shape\n",
    "    inp = Input(shape=input_shape)\n",
    "    g1 = conv2d(inp, filters=hparams.siamese.filter, kernel_size=(h,3), strides=1, padding='valid', sn=False)\n",
    "    g2 = conv2d(g1, filters=hparams.siamese.filter, kernel_size=(1,9), strides=(1,2), sn=False)\n",
    "    g3 = conv2d(g2, filters=hparams.siamese.filter, kernel_size=(1,7), strides=(1,2), sn=False)\n",
    "    g4 = Flatten()(g3)\n",
    "    g5 = Dense(hparams.vec_len)(g4)\n",
    "    return Model(inp, g5, name='S')\n",
    "\n",
    "#Discriminator (Critic) Network\n",
    "def build_critic(input_shape, hparams):\n",
    "    h,w,c = input_shape\n",
    "    inp = Input(shape=input_shape)\n",
    "    g1 = conv2d(inp, filters=hparams.discriminator.filter, kernel_size=(h,3), strides=1, padding='valid', bnorm=False)\n",
    "    g2 = conv2d(g1, filters=hparams.discriminator.filter, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
    "    g3 = conv2d(g2, filters=hparams.discriminator.filter, kernel_size=(1,7), strides=(1,2), bnorm=False)\n",
    "    g4 = Flatten()(g3)\n",
    "    g4 = DenseSN(1, kernel_initializer=tf.keras.initializers.he_uniform())(g4)\n",
    "    return Model(inp, g4, name='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "#Load past models from path to resume training or test\n",
    "def load(path, hparams):\n",
    "    gen = build_generator((hparams.hop, hparams.shape,1), hparams)\n",
    "    siam = build_siamese((hparams.hop, hparams.shape,1), hparams)\n",
    "    critic = build_critic((hparams.hop, 3 * hparams.shape,1), hparams)\n",
    "    \n",
    "    gen.load_weights(path+'/gen.h5')\n",
    "    critic.load_weights(path+'/critic.h5')\n",
    "    siam.load_weights(path+'/siam.h5')\n",
    "    return gen,critic,siam\n",
    "\n",
    "#Build models\n",
    "def build(hparams):\n",
    "    gen = build_generator((hparams.hop, hparams.shape,1), hparams)\n",
    "    siam = build_siamese((hparams.hop, hparams.shape,1), hparams)\n",
    "    critic = build_critic((hparams.hop, 3 * hparams.shape,1), hparams) #the discriminator accepts as input spectrograms of triple the width of those generated by the generator\n",
    "    return gen,critic,siam\n",
    "\n",
    "#Generate a random batch to display current training results\n",
    "def testgena(aspec, hparams):\n",
    "    sw = True\n",
    "    while sw:\n",
    "        a = np.random.choice(aspec)\n",
    "        if a.shape[1]//hparams.shape != 1:\n",
    "            sw=False\n",
    "    \n",
    "    dsa = []\n",
    "    \n",
    "    if a.shape[1]//hparams.shape > 6:\n",
    "        num=6\n",
    "    else:\n",
    "        num=a.shape[1]//hparams.shape\n",
    "\n",
    "    rn = np.random.randint(a.shape[1]-(num * hparams.shape))\n",
    "    \n",
    "    for i in range(num):\n",
    "        im = a[ : , rn + (i * hparams.shape) : rn+(i * hparams.shape) + hparams.shape]\n",
    "        im = np.reshape(im, (im.shape[0], im.shape[1],1))\n",
    "        dsa.append(im)\n",
    "        \n",
    "    return np.array(dsa, dtype=np.float32)\n",
    "\n",
    "#Show results mid-training\n",
    "def save_test_image_full(path, aspec, gen, hparams):\n",
    "    a = testgena(aspec, hparams)\n",
    "\n",
    "    ab = gen(a, training=False)\n",
    "    ab = testass(ab)\n",
    "    a = testass(a)\n",
    "    \n",
    "    abwv = deprep(ab)\n",
    "    awv = deprep(a)\n",
    "    sf.write(path+'/new_file.wav', abwv, sr)\n",
    "    IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
    "    IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
    "    fig, axs = plt.subplots(ncols=2)\n",
    "    axs[0].imshow(np.flip(a, -2), cmap=None)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Source')\n",
    "    axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Generated')\n",
    "    plt.show()\n",
    "\n",
    "#Save in training loop\n",
    "def save_end(epoch,gloss,closs,mloss,n_save=3,save_path='/data/output/models'):                 #use custom save_path (i.e. Drive '../content/drive/My Drive/')\n",
    "    if epoch % n_save == 0:\n",
    "        print('Saving...')\n",
    "        path = f'{save_path}/MELGANVC-{str(gloss)[:9]}-{str(closs)[:9]}-{str(mloss)[:9]}'\n",
    "        os.mkdir(path)\n",
    "        gen.save_weights(path+'/gen.h5')\n",
    "        critic.save_weights(path+'/critic.h5')\n",
    "        siam.save_weights(path+'/siam.h5')\n",
    "        save_test_image_full(path)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
