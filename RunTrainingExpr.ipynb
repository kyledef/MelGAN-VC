{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "\n",
    "hop=192               #hop size (window size = 6*hop)\n",
    "sr=16000              #sampling rate\n",
    "min_level_db=-100     #reference values to normalize data\n",
    "ref_level_db=20\n",
    "\n",
    "shape=24              #length of time axis of split specrograms to feed to generator            \n",
    "vec_len=128           #length of vector generated by siamese vector\n",
    "# bs = 16               #batch size\n",
    "bs = 32               #batch size\n",
    "delta = 2.            #constant for siamese loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(34876, 192, 240, 1)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "brecs = np.load('/data/wav/np/v1/bdata.npz', allow_pickle=True)\n",
    "bdata = brecs['bdata']\n",
    "bdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7827,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bspec = brecs['bspec']\n",
    "bspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39608, 192, 240, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arecs = np.load('/data/wav/np/v1/adata.npz', allow_pickle=True)\n",
    "adata = arecs['adata']\n",
    "adata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7136,)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "aspec = arecs['aspec']\n",
    "aspec.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating Tensorflow Datasets\n",
    "\n",
    "@tf.function\n",
    "def proc(x):\n",
    "    return tf.image.random_crop(x, size=[hop, 3*shape, 1])\n",
    "\n",
    "dsa = tf.data.Dataset.from_tensor_slices(adata).repeat(50).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)\n",
    "dsb = tf.data.Dataset.from_tensor_slices(bdata).repeat(50).map(proc, num_parallel_calls=tf.data.experimental.AUTOTUNE).shuffle(10000).batch(bs, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Helper functions\n",
    "\n",
    "#Generate spectrograms from waveform array\n",
    "def tospec(data):\n",
    "    specs=np.empty(data.shape[0], dtype=object)\n",
    "    for i in range(data.shape[0]):\n",
    "        x = data[i]\n",
    "        S=prep(x)\n",
    "        S = np.array(S, dtype=np.float32)\n",
    "        specs[i]=np.expand_dims(S, -1)\n",
    "    print(specs.shape)\n",
    "    return specs\n",
    "\n",
    "#Generate multiple spectrograms with a determined length from single wav file\n",
    "def tospeclong(path, length=4*16000):\n",
    "    x, sr = librosa.load(path,sr=16000)\n",
    "    x,_ = librosa.effects.trim(x)\n",
    "    loudls = librosa.effects.split(x, top_db=50)\n",
    "    xls = np.array([])\n",
    "    for interv in loudls:\n",
    "        xls = np.concatenate((xls,x[interv[0]:interv[1]]))\n",
    "    x = xls\n",
    "    num = x.shape[0]//length\n",
    "    specs=np.empty(num, dtype=object)\n",
    "    for i in range(num-1):\n",
    "        a = x[i*length:(i+1)*length]\n",
    "        S = prep(a)\n",
    "        S = np.array(S, dtype=np.float32)\n",
    "        try:\n",
    "            sh = S.shape\n",
    "            specs[i]=S\n",
    "        except AttributeError:\n",
    "            print('spectrogram failed')\n",
    "    print(specs.shape)\n",
    "    return specs\n",
    "\n",
    "#Waveform array from path of folder containing wav files\n",
    "def audio_array(path):\n",
    "    ls = glob(f'{path}/*.wav')\n",
    "    adata = []\n",
    "    for i in range(len(ls)):\n",
    "        x, sr = tf.audio.decode_wav(tf.io.read_file(ls[i]), 1)\n",
    "        x = np.array(x, dtype=np.float32)\n",
    "        adata.append(x)\n",
    "    return np.array(adata)\n",
    "\n",
    "#Concatenate spectrograms in array along the time axis\n",
    "def testass(a):\n",
    "    but=False\n",
    "    con = np.array([])\n",
    "    nim = a.shape[0]\n",
    "    for i in range(nim):\n",
    "        im = a[i]\n",
    "        im = np.squeeze(im)\n",
    "        if not but:\n",
    "            con=im\n",
    "            but=True\n",
    "        else:\n",
    "            con = np.concatenate((con,im), axis=1)\n",
    "    return np.squeeze(con)\n",
    "\n",
    "# NB shape=24 #length of time axis of split specrograms to feed to generator   \n",
    "#Split spectrograms in chunks with equal size\n",
    "def splitcut(data, debug=False):\n",
    "    ls = []\n",
    "    mini = data[0].shape[1]\n",
    "    minifinal = 10*shape                                                              #max spectrogram length\n",
    "    for i in range(data.shape[0]-1):\n",
    "        if data[i].shape[1]<=mini:\n",
    "            mini = data[i].shape[1]\n",
    "    \n",
    "    if mini >= 3*shape and mini < minifinal:\n",
    "        minifinal = mini\n",
    "        \n",
    "    if debug:\n",
    "        print(f\"Mininum chunk size is {minifinal}\")\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        x = data[i]\n",
    "        if debug:\n",
    "            print(f\"{i} {x.shape}-> {x.shape[1]} >= {3*shape}?\")\n",
    "            \n",
    "        if x.shape[1]>=3*shape:\n",
    "            if debug:\n",
    "                print(f\" Attempt to break the file up into {x.shape[1]//minifinal} chunks of size {minifinal}\")\n",
    "            \n",
    "            for n in range(x.shape[1]//minifinal):\n",
    "                ls.append(x[:,n*minifinal:n*minifinal+minifinal,:])\n",
    "        ls.append(x[:,-minifinal:,:])\n",
    "    \n",
    "    return np.array(ls)\n",
    "\n",
    "def splitcut_v2(data, debug=False):\n",
    "    ls = []\n",
    "    min_frames = 0\n",
    "    # get the smallest \n",
    "    for i in range(data.shape[0]):\n",
    "        min_frames = min(min_frames, data[i])\n",
    "    \n",
    "    # Ensure that the chunk_size is not larger than the max expected spectrogram length\n",
    "    chunk_size = min(min_frames, 10*shape)\n",
    "    \n",
    "    for i in range(data.shape[0]):\n",
    "        spectrogram  = data[i]\n",
    "        if debug:\n",
    "            print(f\"{i} {spectrogram.shape}-> {spectrogram.shape[1]} >= {chunk_size}?\")\n",
    "        spectrogram_splits = [spectrogram[i * chunk_size: (i+1) * chunk_size] for i in range(int(np.ceil(len(spectrogram) / chunk_size)))]\n",
    "        \n",
    "        ls.append(spectrogram_splits)\n",
    "    return np.array(ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding Spectral Normalization to convolutional layers\n",
    "\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import math_ops\n",
    "from tensorflow.python.ops import sparse_ops\n",
    "from tensorflow.python.ops import gen_math_ops\n",
    "from tensorflow.python.ops import standard_ops\n",
    "from tensorflow.python.eager import context\n",
    "from tensorflow.python.framework import tensor_shape\n",
    "from tensorflow.keras.layers import Input, Dense, Reshape, Flatten, Concatenate, Conv2D, Conv2DTranspose, GlobalAveragePooling2D, UpSampling2D, LeakyReLU, ReLU, Add, Multiply, Lambda, Dot, BatchNormalization, Activation, ZeroPadding2D, Cropping2D, Cropping1D\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import TruncatedNormal, he_normal\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "def l2normalize(v, eps=1e-12):\n",
    "    return v / (tf.norm(v) + eps)\n",
    "\n",
    "\n",
    "class ConvSN2D(tf.keras.layers.Conv2D):\n",
    "\n",
    "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
    "        super(ConvSN2D, self).__init__(filters, kernel_size, **kwargs)\n",
    "        self.power_iterations = power_iterations\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ConvSN2D, self).build(input_shape)\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        self.u = self.add_weight(self.name + '_u',\n",
    "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
    "            initializer=tf.initializers.RandomNormal(0, 1),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
    "        for _ in range(self.power_iterations):\n",
    "\n",
    "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
    "            new_u = l2normalize(tf.matmul(new_v, W))\n",
    "            \n",
    "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
    "        W_bar = W/sigma\n",
    "\n",
    "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
    "            W_bar = tf.reshape(W_bar, W_shape)\n",
    "\n",
    "        return W_bar\n",
    "\n",
    "\n",
    "    def call(self, inputs):\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
    "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
    "        outputs = self._convolution_op(inputs, new_kernel)\n",
    "\n",
    "        if self.use_bias:\n",
    "            if self.data_format == 'channels_first':\n",
    "                    outputs = tf.nn.bias_add(outputs, self.bias, data_format='NCHW')\n",
    "            else:\n",
    "                outputs = tf.nn.bias_add(outputs, self.bias, data_format='NHWC')\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "class ConvSN2DTranspose(tf.keras.layers.Conv2DTranspose):\n",
    "\n",
    "    def __init__(self, filters, kernel_size, power_iterations=1, **kwargs):\n",
    "        super(ConvSN2DTranspose, self).__init__(filters, kernel_size, **kwargs)\n",
    "        self.power_iterations = power_iterations\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(ConvSN2DTranspose, self).build(input_shape)\n",
    "\n",
    "        if self.data_format == 'channels_first':\n",
    "            channel_axis = 1\n",
    "        else:\n",
    "            channel_axis = -1\n",
    "\n",
    "        self.u = self.add_weight(self.name + '_u',\n",
    "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
    "            initializer=tf.initializers.RandomNormal(0, 1),\n",
    "            trainable=False\n",
    "        )\n",
    "\n",
    "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
    "        for _ in range(self.power_iterations):\n",
    "\n",
    "            new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
    "            new_u = l2normalize(tf.matmul(new_v, W))\n",
    "            \n",
    "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
    "        W_bar = W/sigma\n",
    "\n",
    "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
    "            W_bar = tf.reshape(W_bar, W_shape)\n",
    "\n",
    "        return W_bar\n",
    "\n",
    "    def call(self, inputs):\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
    "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
    "\n",
    "        inputs_shape = array_ops.shape(inputs)\n",
    "        batch_size = inputs_shape[0]\n",
    "        if self.data_format == 'channels_first':\n",
    "            h_axis, w_axis = 2, 3\n",
    "        else:\n",
    "            h_axis, w_axis = 1, 2\n",
    "\n",
    "        height, width = inputs_shape[h_axis], inputs_shape[w_axis]\n",
    "        kernel_h, kernel_w = self.kernel_size\n",
    "        stride_h, stride_w = self.strides\n",
    "\n",
    "        if self.output_padding is None:\n",
    "            out_pad_h = out_pad_w = None\n",
    "        else:\n",
    "            out_pad_h, out_pad_w = self.output_padding\n",
    "\n",
    "        out_height = conv_utils.deconv_output_length(height,\n",
    "                                                    kernel_h,\n",
    "                                                    padding=self.padding,\n",
    "                                                    output_padding=out_pad_h,\n",
    "                                                    stride=stride_h,\n",
    "                                                    dilation=self.dilation_rate[0])\n",
    "        out_width = conv_utils.deconv_output_length(width,\n",
    "                                                    kernel_w,\n",
    "                                                    padding=self.padding,\n",
    "                                                    output_padding=out_pad_w,\n",
    "                                                    stride=stride_w,\n",
    "                                                    dilation=self.dilation_rate[1])\n",
    "        if self.data_format == 'channels_first':\n",
    "            output_shape = (batch_size, self.filters, out_height, out_width)\n",
    "        else:\n",
    "            output_shape = (batch_size, out_height, out_width, self.filters)\n",
    "\n",
    "        output_shape_tensor = array_ops.stack(output_shape)\n",
    "        outputs = K.conv2d_transpose(\n",
    "            inputs,\n",
    "            new_kernel,\n",
    "            output_shape_tensor,\n",
    "            strides=self.strides,\n",
    "            padding=self.padding,\n",
    "            data_format=self.data_format,\n",
    "            dilation_rate=self.dilation_rate)\n",
    "\n",
    "        if not context.executing_eagerly():\n",
    "            out_shape = self.compute_output_shape(inputs.shape)\n",
    "            outputs.set_shape(out_shape)\n",
    "\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(\n",
    "              outputs,\n",
    "              self.bias,\n",
    "              data_format=conv_utils.convert_data_format(self.data_format, ndim=4))\n",
    "\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs  \n",
    "\n",
    "class DenseSN(Dense):\n",
    "    def build(self, input_shape):\n",
    "        super(DenseSN, self).build(input_shape)\n",
    "\n",
    "        self.u = self.add_weight(self.name + '_u',\n",
    "            shape=tuple([1, self.kernel.shape.as_list()[-1]]), \n",
    "            initializer=tf.initializers.RandomNormal(0, 1),\n",
    "            trainable=False)\n",
    "        \n",
    "    def compute_spectral_norm(self, W, new_u, W_shape):\n",
    "        new_v = l2normalize(tf.matmul(new_u, tf.transpose(W)))\n",
    "        new_u = l2normalize(tf.matmul(new_v, W))\n",
    "        sigma = tf.matmul(tf.matmul(new_v, W), tf.transpose(new_u))\n",
    "        W_bar = W/sigma\n",
    "        with tf.control_dependencies([self.u.assign(new_u)]):\n",
    "            W_bar = tf.reshape(W_bar, W_shape)\n",
    "        return W_bar\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        W_shape = self.kernel.shape.as_list()\n",
    "        W_reshaped = tf.reshape(self.kernel, (-1, W_shape[-1]))\n",
    "        new_kernel = self.compute_spectral_norm(W_reshaped, self.u, W_shape)\n",
    "        rank = len(inputs.shape)\n",
    "        if rank > 2:\n",
    "            outputs = standard_ops.tensordot(inputs, new_kernel, [[rank - 1], [0]])\n",
    "            if not context.executing_eagerly():\n",
    "                shape = inputs.shape.as_list()\n",
    "                output_shape = shape[:-1] + [self.units]\n",
    "                outputs.set_shape(output_shape)\n",
    "        else:\n",
    "            inputs = math_ops.cast(inputs, self._compute_dtype)\n",
    "            if K.is_sparse(inputs):\n",
    "                outputs = sparse_ops.sparse_tensor_dense_matmul(inputs, new_kernel)\n",
    "            else:\n",
    "                outputs = gen_math_ops.mat_mul(inputs, new_kernel)\n",
    "        if self.use_bias:\n",
    "            outputs = tf.nn.bias_add(outputs, self.bias)\n",
    "        if self.activation is not None:\n",
    "            return self.activation(outputs)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Networks Architecture\n",
    "\n",
    "init = tf.keras.initializers.he_uniform()\n",
    "\n",
    "def conv2d(layer_input, filters, kernel_size=4, strides=2, padding='same', leaky=True, bnorm=True, sn=True):\n",
    "    if leaky:\n",
    "        Activ = LeakyReLU(alpha=0.2)\n",
    "    else:\n",
    "        Activ = ReLU()\n",
    "    if sn:\n",
    "        d = ConvSN2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
    "    else:\n",
    "        d = Conv2D(filters, kernel_size=kernel_size, strides=strides, padding=padding, kernel_initializer=init, use_bias=False)(layer_input)\n",
    "    if bnorm:\n",
    "        d = BatchNormalization()(d)\n",
    "    d = Activ(d)\n",
    "    return d\n",
    "\n",
    "def deconv2d(layer_input, layer_res, filters, kernel_size=4, conc=True, scalev=False, bnorm=True, up=True, padding='same', strides=2):\n",
    "    if up:\n",
    "        u = UpSampling2D((1,2))(layer_input)\n",
    "        u = ConvSN2D(filters, kernel_size, strides=(1,1), kernel_initializer=init, use_bias=False, padding=padding)(u)\n",
    "    else:\n",
    "        u = ConvSN2DTranspose(filters, kernel_size, strides=strides, kernel_initializer=init, use_bias=False, padding=padding)(layer_input)\n",
    "    if bnorm:\n",
    "        u = BatchNormalization()(u)\n",
    "    u = LeakyReLU(alpha=0.2)(u)\n",
    "    if conc:\n",
    "        u = Concatenate()([u,layer_res])\n",
    "    return u\n",
    "\n",
    "#Extract function: splitting spectrograms\n",
    "def extract_image(im):\n",
    "    im1 = Cropping2D(((0,0), (0, 2*(im.shape[2]//3))))(im)\n",
    "    im2 = Cropping2D(((0,0), (im.shape[2]//3,im.shape[2]//3)))(im)\n",
    "    im3 = Cropping2D(((0,0), (2*(im.shape[2]//3), 0)))(im)\n",
    "    return im1,im2,im3\n",
    "\n",
    "#Assemble function: concatenating spectrograms\n",
    "def assemble_image(lsim):\n",
    "    im1,im2,im3 = lsim\n",
    "    imh = Concatenate(2)([im1,im2,im3])\n",
    "    return imh\n",
    "\n",
    "#U-NET style architecture\n",
    "def build_generator(input_shape):\n",
    "    h,w,c = input_shape\n",
    "    inp = Input(shape=input_shape)\n",
    "    #downscaling\n",
    "    g0 = tf.keras.layers.ZeroPadding2D((0,1))(inp)\n",
    "    g1 = conv2d(g0, 256, kernel_size=(h,3), strides=1, padding='valid')\n",
    "    g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2))\n",
    "    g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2))\n",
    "    #upscaling\n",
    "    g4 = deconv2d(g3,g2, 256, kernel_size=(1,7), strides=(1,2))\n",
    "    g5 = deconv2d(g4,g1, 256, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
    "    g6 = ConvSN2DTranspose(1, kernel_size=(h,1), strides=(1,1), kernel_initializer=init, padding='valid', activation='tanh')(g5)\n",
    "    return Model(inp,g6, name='G')\n",
    "\n",
    "#Siamese Network\n",
    "def build_siamese(input_shape):\n",
    "    h,w,c = input_shape\n",
    "    inp = Input(shape=input_shape)\n",
    "    g1 = conv2d(inp, 256, kernel_size=(h,3), strides=1, padding='valid', sn=False)\n",
    "    g2 = conv2d(g1, 256, kernel_size=(1,9), strides=(1,2), sn=False)\n",
    "    g3 = conv2d(g2, 256, kernel_size=(1,7), strides=(1,2), sn=False)\n",
    "    g4 = Flatten()(g3)\n",
    "    g5 = Dense(vec_len)(g4)\n",
    "    return Model(inp, g5, name='S')\n",
    "\n",
    "#Discriminator (Critic) Network\n",
    "def build_critic(input_shape):\n",
    "    h,w,c = input_shape\n",
    "    inp = Input(shape=input_shape)\n",
    "    g1 = conv2d(inp, 512, kernel_size=(h,3), strides=1, padding='valid', bnorm=False)\n",
    "    g2 = conv2d(g1, 512, kernel_size=(1,9), strides=(1,2), bnorm=False)\n",
    "    g3 = conv2d(g2, 512, kernel_size=(1,7), strides=(1,2), bnorm=False)\n",
    "    g4 = Flatten()(g3)\n",
    "    g4 = DenseSN(1, kernel_initializer=init)(g4)\n",
    "    return Model(inp, g4, name='C')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load past models from path to resume training or test\n",
    "def load(path):\n",
    "    gen = build_generator((hop,shape,1))\n",
    "    siam = build_siamese((hop,shape,1))\n",
    "    critic = build_critic((hop,3*shape,1))\n",
    "    gen.load_weights(path+'/gen.h5')\n",
    "    critic.load_weights(path+'/critic.h5')\n",
    "    siam.load_weights(path+'/siam.h5')\n",
    "    return gen,critic,siam\n",
    "\n",
    "#Build models\n",
    "def build():\n",
    "    gen = build_generator((hop,shape,1))\n",
    "    siam = build_siamese((hop,shape,1))\n",
    "    critic = build_critic((hop,3*shape,1))                                          #the discriminator accepts as input spectrograms of triple the width of those generated by the generator\n",
    "    return gen,critic,siam\n",
    "\n",
    "#Generate a random batch to display current training results\n",
    "def testgena():\n",
    "    sw = True\n",
    "    while sw:\n",
    "        a = np.random.choice(aspec)\n",
    "        if a.shape[1]//shape!=1:\n",
    "            sw=False\n",
    "    dsa = []\n",
    "    if a.shape[1]//shape>6:\n",
    "        num=6\n",
    "    else:\n",
    "        num=a.shape[1]//shape\n",
    "    rn = np.random.randint(a.shape[1]-(num*shape))\n",
    "    for i in range(num):\n",
    "        im = a[:,rn+(i*shape):rn+(i*shape)+shape]\n",
    "        im = np.reshape(im, (im.shape[0],im.shape[1],1))\n",
    "        dsa.append(im)\n",
    "    return np.array(dsa, dtype=np.float32)\n",
    "\n",
    "#Show results mid-training\n",
    "def save_test_image_full(path):\n",
    "    a = testgena()\n",
    "    print(a.shape)\n",
    "    ab = gen(a, training=False)\n",
    "    ab = testass(ab)\n",
    "    a = testass(a)\n",
    "    abwv = deprep(ab)\n",
    "    awv = deprep(a)\n",
    "    sf.write(path+'/new_file.wav', abwv, sr)\n",
    "    IPython.display.display(IPython.display.Audio(np.squeeze(abwv), rate=sr))\n",
    "    IPython.display.display(IPython.display.Audio(np.squeeze(awv), rate=sr))\n",
    "    fig, axs = plt.subplots(ncols=2)\n",
    "    axs[0].imshow(np.flip(a, -2), cmap=None)\n",
    "    axs[0].axis('off')\n",
    "    axs[0].set_title('Source')\n",
    "    axs[1].imshow(np.flip(ab, -2), cmap=None)\n",
    "    axs[1].axis('off')\n",
    "    axs[1].set_title('Generated')\n",
    "    plt.show()\n",
    "\n",
    "#Save in training loop\n",
    "def save_end(epoch,gloss,closs,mloss,n_save=3,save_path='/data/output/models'):                 #use custom save_path (i.e. Drive '../content/drive/My Drive/')\n",
    "    if epoch % n_save == 0:\n",
    "        print('Saving...')\n",
    "        path = f'{save_path}/MELGANVC-{str(gloss)[:9]}-{str(closs)[:9]}-{str(mloss)[:9]}'\n",
    "        os.mkdir(path)\n",
    "        gen.save_weights(path+'/gen.h5')\n",
    "        critic.save_weights(path+'/critic.h5')\n",
    "        siam.save_weights(path+'/siam.h5')\n",
    "        save_test_image_full(path)\n",
    "        \n",
    "#Losses\n",
    "\n",
    "def mae(x,y):\n",
    "    return tf.reduce_mean(tf.abs(x-y))\n",
    "\n",
    "def mse(x,y):\n",
    "    return tf.reduce_mean((x-y)**2)\n",
    "\n",
    "def loss_travel(sa,sab,sa1,sab1):\n",
    "    l1 = tf.reduce_mean(((sa-sa1) - (sab-sab1))**2)\n",
    "    l2 = tf.reduce_mean(tf.reduce_sum(-(tf.nn.l2_normalize(sa-sa1, axis=[-1]) * tf.nn.l2_normalize(sab-sab1, axis=[-1])), axis=-1))\n",
    "    return l1+l2\n",
    "\n",
    "def loss_siamese(sa,sa1):\n",
    "    logits = tf.sqrt(tf.reduce_sum((sa-sa1)**2, axis=-1, keepdims=True))\n",
    "    return tf.reduce_mean(tf.square(tf.maximum((delta - logits), 0)))\n",
    "\n",
    "def d_loss_f(fake):\n",
    "    return tf.reduce_mean(tf.maximum(1 + fake, 0))\n",
    "\n",
    "def d_loss_r(real):\n",
    "    return tf.reduce_mean(tf.maximum(1 - real, 0))\n",
    "\n",
    "def g_loss_f(fake):\n",
    "    return tf.reduce_mean(- fake)\n",
    "\n",
    "#Get models and optimizers\n",
    "def get_networks(shape, load_model=False, path=None):\n",
    "    if not load_model:\n",
    "        gen,critic,siam = build()\n",
    "    else:\n",
    "        gen,critic,siam = load(path)\n",
    "\n",
    "    opt_gen = Adam(0.0001, 0.5)\n",
    "    opt_disc = Adam(0.0001, 0.5)\n",
    "\n",
    "    return gen,critic,siam, [opt_gen,opt_disc]\n",
    "\n",
    "#Set learning rate\n",
    "def update_lr(lr):\n",
    "    opt_gen.learning_rate = lr\n",
    "    opt_disc.learning_rate = lr\n",
    "\n",
    "#Training Functions\n",
    "\n",
    "#Train Generator, Siamese and Critic\n",
    "@tf.function\n",
    "def train_all(a,b):\n",
    "    #splitting spectrogram in 3 parts\n",
    "    aa,aa2,aa3 = extract_image(a) \n",
    "    bb,bb2,bb3 = extract_image(b)\n",
    "\n",
    "    with tf.GradientTape() as tape_gen, tf.GradientTape() as tape_disc:\n",
    "        #translating A to B\n",
    "        fab = gen(aa, training=True)\n",
    "        fab2 = gen(aa2, training=True)\n",
    "        fab3 = gen(aa3, training=True)\n",
    "        #identity mapping B to B                                                        COMMENT THESE 3 LINES IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
    "        fid = gen(bb, training=True) \n",
    "        fid2 = gen(bb2, training=True)\n",
    "        fid3 = gen(bb3, training=True)\n",
    "        #concatenate/assemble converted spectrograms\n",
    "        fabtot = assemble_image([fab,fab2,fab3])\n",
    "        #feed concatenated spectrograms to critic\n",
    "        cab = critic(fabtot, training=True)\n",
    "        cb = critic(b, training=True)\n",
    "        #feed 2 pairs (A,G(A)) extracted spectrograms to Siamese\n",
    "        sab = siam(fab, training=True)\n",
    "        sab2 = siam(fab3, training=True)\n",
    "        sa = siam(aa, training=True)\n",
    "        sa2 = siam(aa3, training=True)\n",
    "        #identity mapping loss\n",
    "        loss_id = (mae(bb,fid)+mae(bb2,fid2)+mae(bb3,fid3))/3.                         #loss_id = 0. IF THE IDENTITY LOSS TERM IS NOT NEEDED\n",
    "        #travel loss\n",
    "        loss_m = loss_travel(sa,sab,sa2,sab2)+loss_siamese(sa,sa2)\n",
    "        #generator and critic losses\n",
    "        loss_g = g_loss_f(cab)\n",
    "        loss_dr = d_loss_r(cb)\n",
    "        loss_df = d_loss_f(cab)\n",
    "        loss_d = (loss_dr+loss_df)/2.\n",
    "        #generator+siamese total loss\n",
    "        lossgtot = loss_g+10.*loss_m+0.5*loss_id                                       #CHANGE LOSS WEIGHTS HERE  (COMMENT OUT +w*loss_id IF THE IDENTITY LOSS TERM IS NOT NEEDED)\n",
    "  \n",
    "    #computing and applying gradients\n",
    "    grad_gen = tape_gen.gradient(lossgtot, gen.trainable_variables+siam.trainable_variables)\n",
    "    opt_gen.apply_gradients(zip(grad_gen, gen.trainable_variables+siam.trainable_variables))\n",
    "\n",
    "    grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
    "    opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
    "\n",
    "    return loss_dr,loss_df,loss_g,loss_id\n",
    "\n",
    "#Train Critic only\n",
    "@tf.function\n",
    "def train_d(a,b):\n",
    "    aa,aa2,aa3 = extract_image(a)\n",
    "    with tf.GradientTape() as tape_disc:\n",
    "\n",
    "        fab = gen(aa, training=True)\n",
    "        fab2 = gen(aa2, training=True)\n",
    "        fab3 = gen(aa3, training=True)\n",
    "        fabtot = assemble_image([fab,fab2,fab3])\n",
    "\n",
    "        cab = critic(fabtot, training=True)\n",
    "        cb = critic(b, training=True)\n",
    "\n",
    "        loss_dr = d_loss_r(cb)\n",
    "        loss_df = d_loss_f(cab)\n",
    "\n",
    "        loss_d = (loss_dr+loss_df)/2.\n",
    "  \n",
    "    grad_disc = tape_disc.gradient(loss_d, critic.trainable_variables)\n",
    "    opt_disc.apply_gradients(zip(grad_disc, critic.trainable_variables))\n",
    "\n",
    "    return loss_dr,loss_df\n",
    "\n",
    "def train(epochs, batch_size=16, lr=0.0001, n_save=6, gupt=5, dsa =[], dsb=[], save_path=''):\n",
    "    import time\n",
    "    update_lr(lr)\n",
    "    df_list = []\n",
    "    dr_list = []\n",
    "    g_list = []\n",
    "    id_list = []\n",
    "    c = 0\n",
    "    g = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        bef = time.time()\n",
    "        \n",
    "        for batchi,(a,b) in enumerate(zip(dsa,dsb)):\n",
    "          \n",
    "            if batchi % gupt==0:\n",
    "                dloss_t,dloss_f,gloss,idloss = train_all(a,b)\n",
    "            else:\n",
    "                dloss_t,dloss_f = train_d(a,b)\n",
    "\n",
    "            df_list.append(dloss_f)\n",
    "            dr_list.append(dloss_t)\n",
    "            g_list.append(gloss)\n",
    "            id_list.append(idloss)\n",
    "            c += 1\n",
    "            g += 1\n",
    "\n",
    "            if batchi%600==0:\n",
    "                print(f'[Epoch {epoch}/{epochs}] [Batch {batchi}] [D loss f: {np.mean(df_list[-g:], axis=0)} ', end='')\n",
    "                print(f'r: {np.mean(dr_list[-g:], axis=0)}] ', end='')\n",
    "                print(f'[G loss: {np.mean(g_list[-g:], axis=0)}] ', end='')\n",
    "                print(f'[ID loss: {np.mean(id_list[-g:])}] ', end='')\n",
    "                print(f'[LR: {lr}]')\n",
    "                g = 0\n",
    "            nbatch=batchi\n",
    "\n",
    "        print(f'Completed epoch {epoch}/{epochs} /t Time/Batch {(time.time()-bef)/nbatch}')\n",
    "        save_end(epoch,np.mean(g_list[-n_save*c:], axis=0),np.mean(df_list[-n_save*c:], axis=0),np.mean(id_list[-n_save*c:], axis=0),n_save=n_save, save_path=save_path)\n",
    "        print(f'Mean D loss: {np.mean(df_list[-c:], axis=0)} Mean G loss: {np.mean(g_list[-c:], axis=0)} Mean ID loss: {np.mean(id_list[-c:], axis=0)}')\n",
    "        c = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "exprDirName=\"/data/output/models/exp5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  /data/output/models/exp5  Created \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "try:\n",
    "    os.makedirs(exprDirName)    \n",
    "    print(\"Directory \" , exprDirName ,  \" Created \")\n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , exprDirName ,  \" already exists\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen,critic,siam, [opt_gen,opt_disc] = get_networks(shape, load_model=False, path=exprDirName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 0/5] [Batch 0] [D loss f: 0.9928476810455322 r: 1.0580847263336182] [G loss: 0.007152269594371319] [ID loss: 0.6795355081558228] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 600] [D loss f: 0.06871896982192993 r: 0.02081494964659214] [G loss: 1.42197847366333] [ID loss: 0.6884912848472595] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 1200] [D loss f: 0.00020646884513553232 r: 3.463685425231233e-05] [G loss: 1.6386712789535522] [ID loss: 0.6647340655326843] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 1800] [D loss f: 0.0002417055075056851 r: 3.729834497789852e-05] [G loss: 1.5521677732467651] [ID loss: 0.6430549621582031] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 2400] [D loss f: 0.00019096990581601858 r: 9.76934825303033e-05] [G loss: 1.52241051197052] [ID loss: 0.6007041335105896] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 3000] [D loss f: 0.000835413346067071 r: 0.0005725978408008814] [G loss: 1.690484881401062] [ID loss: 0.5470640659332275] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 3600] [D loss f: 0.0062784901820123196 r: 0.003390100784599781] [G loss: 1.59686279296875] [ID loss: 0.4906673729419708] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 4200] [D loss f: 0.010246848687529564 r: 0.014533079229295254] [G loss: 1.3993138074874878] [ID loss: 0.44734305143356323] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 4800] [D loss f: 0.041790980845689774 r: 0.06676248461008072] [G loss: 1.2543413639068604] [ID loss: 0.40504342317581177] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 5400] [D loss f: 0.125925675034523 r: 0.17105236649513245] [G loss: 1.0436221361160278] [ID loss: 0.35875648260116577] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 6000] [D loss f: 0.21176455914974213 r: 0.25856488943099976] [G loss: 0.9102100729942322] [ID loss: 0.3174068331718445] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 6600] [D loss f: 0.23551945388317108 r: 0.2740638554096222] [G loss: 0.8462439179420471] [ID loss: 0.2919706106185913] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 7200] [D loss f: 0.3152295649051666 r: 0.31109538674354553] [G loss: 0.766541063785553] [ID loss: 0.2681325674057007] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 7800] [D loss f: 0.3727376163005829 r: 0.38315337896347046] [G loss: 0.669118344783783] [ID loss: 0.253241628408432] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 8400] [D loss f: 0.38740214705467224 r: 0.4112740457057953] [G loss: 0.674720048904419] [ID loss: 0.2425091564655304] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 9000] [D loss f: 0.44982463121414185 r: 0.43711715936660767] [G loss: 0.5909992456436157] [ID loss: 0.23254714906215668] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 9600] [D loss f: 0.5626864433288574 r: 0.46977996826171875] [G loss: 0.4636786878108978] [ID loss: 0.22634664177894592] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 10200] [D loss f: 0.6095439791679382 r: 0.49557769298553467] [G loss: 0.4273265600204468] [ID loss: 0.2174968272447586] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 10800] [D loss f: 0.6104360222816467 r: 0.500927209854126] [G loss: 0.4152555465698242] [ID loss: 0.2121257483959198] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 11400] [D loss f: 0.6386311650276184 r: 0.47173574566841125] [G loss: 0.3685171902179718] [ID loss: 0.20246268808841705] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 12000] [D loss f: 0.676426112651825 r: 0.46484288573265076] [G loss: 0.3727863132953644] [ID loss: 0.20189280807971954] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 12600] [D loss f: 0.6983363628387451 r: 0.4669630825519562] [G loss: 0.3449438810348511] [ID loss: 0.20007406175136566] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 13200] [D loss f: 0.7158561944961548 r: 0.46734070777893066] [G loss: 0.31465214490890503] [ID loss: 0.19630083441734314] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 13800] [D loss f: 0.7041932344436646 r: 0.4708317518234253] [G loss: 0.33220815658569336] [ID loss: 0.19979581236839294] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 14400] [D loss f: 0.7209079265594482 r: 0.46735575795173645] [G loss: 0.3012920618057251] [ID loss: 0.19478724896907806] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 15000] [D loss f: 0.7168315649032593 r: 0.46153706312179565] [G loss: 0.31876006722450256] [ID loss: 0.19393694400787354] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 15600] [D loss f: 0.731218159198761 r: 0.47416290640830994] [G loss: 0.281038373708725] [ID loss: 0.19125127792358398] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 16200] [D loss f: 0.7388988137245178 r: 0.48989614844322205] [G loss: 0.2920753061771393] [ID loss: 0.19087007641792297] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 16800] [D loss f: 0.7762543559074402 r: 0.49025797843933105] [G loss: 0.2342093139886856] [ID loss: 0.18375493586063385] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 17400] [D loss f: 0.7703802585601807 r: 0.49245208501815796] [G loss: 0.2613525390625] [ID loss: 0.18351246416568756] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 18000] [D loss f: 0.7691812515258789 r: 0.49792709946632385] [G loss: 0.26190629601478577] [ID loss: 0.17760060727596283] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 18600] [D loss f: 0.7800697088241577 r: 0.5028032064437866] [G loss: 0.23595698177814484] [ID loss: 0.17641232907772064] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 19200] [D loss f: 0.8007090091705322 r: 0.48944732546806335] [G loss: 0.2168685644865036] [ID loss: 0.1752060502767563] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 19800] [D loss f: 0.807007372379303 r: 0.4907098412513733] [G loss: 0.20471231639385223] [ID loss: 0.17584431171417236] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 20400] [D loss f: 0.8070477247238159 r: 0.500866174697876] [G loss: 0.21134410798549652] [ID loss: 0.1737586408853531] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 21000] [D loss f: 0.8289852738380432 r: 0.4810694754123688] [G loss: 0.17317253351211548] [ID loss: 0.16880828142166138] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 21600] [D loss f: 0.8301777243614197 r: 0.4908791184425354] [G loss: 0.18139860033988953] [ID loss: 0.1664363592863083] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 22200] [D loss f: 0.8488625884056091 r: 0.5009042620658875] [G loss: 0.15998801589012146] [ID loss: 0.1672324687242508] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 22800] [D loss f: 0.8317638039588928 r: 0.503711462020874] [G loss: 0.1671803742647171] [ID loss: 0.16452664136886597] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 23400] [D loss f: 0.8433093428611755 r: 0.5020538568496704] [G loss: 0.17349417507648468] [ID loss: 0.16381771862506866] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 24000] [D loss f: 0.8289032578468323 r: 0.5024967193603516] [G loss: 0.22309623658657074] [ID loss: 0.16167575120925903] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 24600] [D loss f: 0.8443691730499268 r: 0.4939521253108978] [G loss: 0.18654270470142365] [ID loss: 0.16005048155784607] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 25200] [D loss f: 0.8300309777259827 r: 0.5197389125823975] [G loss: 0.19606231153011322] [ID loss: 0.15715083479881287] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 25800] [D loss f: 0.8237305879592896 r: 0.5210288166999817] [G loss: 0.16923388838768005] [ID loss: 0.1536158174276352] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 26400] [D loss f: 0.848605751991272 r: 0.4962086081504822] [G loss: 0.1740117222070694] [ID loss: 0.1525074690580368] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 27000] [D loss f: 0.8406859636306763 r: 0.4995861351490021] [G loss: 0.17394378781318665] [ID loss: 0.15200579166412354] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 27600] [D loss f: 0.8198161721229553 r: 0.5141434669494629] [G loss: 0.1902695745229721] [ID loss: 0.1519368439912796] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 28200] [D loss f: 0.8297395706176758 r: 0.5018743872642517] [G loss: 0.22017204761505127] [ID loss: 0.1515205055475235] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 28800] [D loss f: 0.8114672303199768 r: 0.5168986916542053] [G loss: 0.22829970717430115] [ID loss: 0.15013034641742706] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 29400] [D loss f: 0.8120077252388 r: 0.5202589631080627] [G loss: 0.209194615483284] [ID loss: 0.14884412288665771] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 30000] [D loss f: 0.8085301518440247 r: 0.5240721702575684] [G loss: 0.20457515120506287] [ID loss: 0.14932751655578613] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 30600] [D loss f: 0.8136147260665894 r: 0.515701174736023] [G loss: 0.22214393317699432] [ID loss: 0.15049108862876892] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 31200] [D loss f: 0.805310845375061 r: 0.5225547552108765] [G loss: 0.19693094491958618] [ID loss: 0.15045778453350067] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 31800] [D loss f: 0.8160430192947388 r: 0.5123072862625122] [G loss: 0.201348677277565] [ID loss: 0.14801834523677826] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 32400] [D loss f: 0.7988244891166687 r: 0.5303431153297424] [G loss: 0.2340693473815918] [ID loss: 0.1465279906988144] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 33000] [D loss f: 0.8006540536880493 r: 0.5149179697036743] [G loss: 0.21053676307201385] [ID loss: 0.14754627645015717] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 33600] [D loss f: 0.7975627779960632 r: 0.5261160135269165] [G loss: 0.2104235142469406] [ID loss: 0.14458735287189484] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 34200] [D loss f: 0.7967756390571594 r: 0.5221689343452454] [G loss: 0.2365059107542038] [ID loss: 0.14523249864578247] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 34800] [D loss f: 0.7878619432449341 r: 0.5273798108100891] [G loss: 0.25996217131614685] [ID loss: 0.14528538286685944] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 35400] [D loss f: 0.7883044481277466 r: 0.5309491753578186] [G loss: 0.2422657012939453] [ID loss: 0.14512208104133606] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 36000] [D loss f: 0.7803925275802612 r: 0.525513231754303] [G loss: 0.24563908576965332] [ID loss: 0.14562253654003143] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 36600] [D loss f: 0.7817636132240295 r: 0.5250846743583679] [G loss: 0.22602351009845734] [ID loss: 0.14610208570957184] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 37200] [D loss f: 0.7886056900024414 r: 0.5225458741188049] [G loss: 0.1996382176876068] [ID loss: 0.1445271521806717] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 37800] [D loss f: 0.7902135252952576 r: 0.5365911722183228] [G loss: 0.2261168211698532] [ID loss: 0.14371950924396515] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 38400] [D loss f: 0.7812848091125488 r: 0.5306374430656433] [G loss: 0.23582667112350464] [ID loss: 0.14341695606708527] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 39000] [D loss f: 0.7850720286369324 r: 0.5238447785377502] [G loss: 0.256233274936676] [ID loss: 0.14110396802425385] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 39600] [D loss f: 0.7840633392333984 r: 0.5341273546218872] [G loss: 0.2556537687778473] [ID loss: 0.1411338895559311] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 40200] [D loss f: 0.7828534841537476 r: 0.5392239689826965] [G loss: 0.23776648938655853] [ID loss: 0.14062519371509552] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 40800] [D loss f: 0.789867639541626 r: 0.532301664352417] [G loss: 0.24844136834144592] [ID loss: 0.1395712047815323] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 41400] [D loss f: 0.7904173731803894 r: 0.5367339849472046] [G loss: 0.22358591854572296] [ID loss: 0.13915543258190155] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 42000] [D loss f: 0.7858569025993347 r: 0.541676938533783] [G loss: 0.24646946787834167] [ID loss: 0.13735367357730865] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 42600] [D loss f: 0.7654657959938049 r: 0.5460374355316162] [G loss: 0.27926695346832275] [ID loss: 0.13819767534732819] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 43200] [D loss f: 0.771737277507782 r: 0.547266960144043] [G loss: 0.25661951303482056] [ID loss: 0.13641759753227234] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 43800] [D loss f: 0.7799152135848999 r: 0.5414724946022034] [G loss: 0.2504473626613617] [ID loss: 0.13460657000541687] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 44400] [D loss f: 0.7675846815109253 r: 0.5527040362358093] [G loss: 0.2590947449207306] [ID loss: 0.1341831386089325] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 45000] [D loss f: 0.7637717127799988 r: 0.5467836260795593] [G loss: 0.28029659390449524] [ID loss: 0.1347539722919464] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 45600] [D loss f: 0.7588878870010376 r: 0.5567672848701477] [G loss: 0.2940327525138855] [ID loss: 0.1346590369939804] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 46200] [D loss f: 0.7428688406944275 r: 0.5628726482391357] [G loss: 0.3049873411655426] [ID loss: 0.13323518633842468] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 46800] [D loss f: 0.7437120676040649 r: 0.5571790337562561] [G loss: 0.3016294240951538] [ID loss: 0.13259094953536987] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 47400] [D loss f: 0.7453670501708984 r: 0.5592612624168396] [G loss: 0.320356547832489] [ID loss: 0.13210591673851013] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 48000] [D loss f: 0.7296217083930969 r: 0.5686704516410828] [G loss: 0.2922212481498718] [ID loss: 0.1329554170370102] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 48600] [D loss f: 0.7475447058677673 r: 0.5660713911056519] [G loss: 0.28395944833755493] [ID loss: 0.12992043793201447] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 49200] [D loss f: 0.7364273071289062 r: 0.56841641664505] [G loss: 0.32087987661361694] [ID loss: 0.12911228835582733] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 49800] [D loss f: 0.7376835942268372 r: 0.5639626979827881] [G loss: 0.28225192427635193] [ID loss: 0.12861411273479462] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 50400] [D loss f: 0.7298982739448547 r: 0.5709543228149414] [G loss: 0.2978304624557495] [ID loss: 0.12757277488708496] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 51000] [D loss f: 0.7411167621612549 r: 0.562882661819458] [G loss: 0.2936251163482666] [ID loss: 0.12965059280395508] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 51600] [D loss f: 0.7293335199356079 r: 0.5744022130966187] [G loss: 0.2902734577655792] [ID loss: 0.1277303397655487] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 52200] [D loss f: 0.7392196655273438 r: 0.5626756548881531] [G loss: 0.31024643778800964] [ID loss: 0.12759889662265778] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 52800] [D loss f: 0.7333412766456604 r: 0.5735596418380737] [G loss: 0.313686341047287] [ID loss: 0.12650328874588013] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 53400] [D loss f: 0.7277013063430786 r: 0.5698774456977844] [G loss: 0.29307809472084045] [ID loss: 0.12541158497333527] [LR: 1e-05]\n",
      "[Epoch 0/5] [Batch 54000] [D loss f: 0.7379882335662842 r: 0.5665589570999146] [G loss: 0.31497085094451904] [ID loss: 0.12568964064121246] [LR: 1e-05]\n",
      "Completed epoch 0/5 /t Time/Batch 0.02736597288935088\n",
      "Saving...\n",
      "(6, 192, 24, 1)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'testass' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-7964ee9fc3b4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#gupt = how many discriminator updates for generator+siamese update\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.00001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgupt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsa\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdsa\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsb\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdsb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexprDirName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-44f7cda479c6>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epochs, batch_size, lr, n_save, gupt, dsa, dsb, save_path)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    221\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Completed epoch {epoch}/{epochs} /t Time/Batch {(time.time()-bef)/nbatch}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 222\u001b[0;31m         \u001b[0msave_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_save\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_save\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mn_save\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_save\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_save\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    223\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Mean D loss: {np.mean(df_list[-c:], axis=0)} Mean G loss: {np.mean(g_list[-c:], axis=0)} Mean ID loss: {np.mean(id_list[-c:], axis=0)}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m         \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-44f7cda479c6>\u001b[0m in \u001b[0;36msave_end\u001b[0;34m(epoch, gloss, closs, mloss, n_save, save_path)\u001b[0m\n\u001b[1;32m     65\u001b[0m         \u001b[0mcritic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/critic.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m         \u001b[0msiam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'/siam.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0msave_test_image_full\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;31m#Losses\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-18-44f7cda479c6>\u001b[0m in \u001b[0;36msave_test_image_full\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mab\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtestass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mabwv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeprep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mab\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'testass' is not defined"
     ]
    }
   ],
   "source": [
    "#Training\n",
    "\n",
    "#n_save = how many epochs between each saving and displaying of results\n",
    "#gupt = how many discriminator updates for generator+siamese update\n",
    "\n",
    "train(5, batch_size=bs, lr=0.00001, n_save=10, gupt=3, dsa=dsa, dsb=dsb, save_path=exprDirName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-1.m47",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m47"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
